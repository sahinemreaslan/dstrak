# DSTARK - DINOv3-based STARK Tracker

DSTARK is a visual object tracker that combines the STARK tracking architecture with DINOv3 backbone, enabling flexible template and search region sizes without hardcoded constraints.

## Key Features

ðŸš€ **Flexible Input Sizes**: Unlike traditional trackers with fixed 128Ã—128 template and 256Ã—256 search sizes, DSTARK supports dynamic sizing thanks to DINOv3's RoPE (Rotary Position Embeddings).

ðŸŽ¯ **Better Occlusion Handling**: DINOv3's rich feature extraction helps maintain target identity even during occlusions.

ðŸ“ˆ **Adaptive Scaling**: Automatically adjusts template and search region sizes based on object scale.

ðŸ’ª **Robust Tracking**: Handles varying object sizes throughout video sequences.

## Architecture

```
DSTARK
â”œâ”€â”€ DINOv3 Backbone (Small)
â”‚   â”œâ”€â”€ Patch size: 16Ã—16
â”‚   â”œâ”€â”€ Embedding dim: 384
â”‚   â”œâ”€â”€ Depth: 12 layers
â”‚   â””â”€â”€ RoPE for flexible sizing
â”‚
â””â”€â”€ Correlation Head
    â”œâ”€â”€ Feature projection
    â”œâ”€â”€ Template-search correlation
    â”œâ”€â”€ Bounding box regression
    â””â”€â”€ Confidence prediction
```

## Installation

```bash
# Clone repository
git clone https://github.com/sahinemreaslan/dstrak.git
cd dstrak

# Install dependencies
pip install -r requirements.txt
```

## Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- OpenCV
- NumPy
- tqdm
- PyYAML

## Dataset Preparation

### GOT-10k

```bash
# Download GOT-10k dataset
mkdir -p data/GOT10k
cd data/GOT10k

# Download from http://got-10k.aitestunion.com/downloads
# Extract train and test splits

# Expected structure:
# data/GOT10k/
#   â”œâ”€â”€ train/
#   â”‚   â”œâ”€â”€ GOT-10k_Train_000001/
#   â”‚   â”œâ”€â”€ GOT-10k_Train_000002/
#   â”‚   â””â”€â”€ ...
#   â””â”€â”€ test/
#       â”œâ”€â”€ GOT-10k_Test_000001/
#       â””â”€â”€ ...
```

## Pretrained Weights

Download DINOv3 small pretrained weights:

```bash
# DINOv3 small weights (already included in repo)
# dinov3_vits16_pretrain.pth
```

## Training

```bash
# Train DSTARK on GOT-10k
python train.py \
    --config configs/dstark_train.yaml \
    --data_root data/GOT10k \
    --output_dir output/dstark \
    --pretrained dinov3_vits16_pretrain.pth \
    --epochs 300 \
    --batch_size 16 \
    --gpu 0

# Resume training from checkpoint
python train.py \
    --config configs/dstark_train.yaml \
    --resume output/dstark/checkpoint_epoch_100.pth
```

### Training Configuration

Key parameters in `configs/dstark_train.yaml`:

- `template_size`: Base template size (default: 128)
- `search_size`: Base search size (default: 256)
- `max_template_size`: Maximum template size (default: 256)
- `max_search_size`: Maximum search size (default: 512)
- `backbone_lr`: Learning rate for backbone (default: 1e-5)
- `head_lr`: Learning rate for tracking head (default: 1e-4)

## Testing

```bash
# Test on GOT-10k
python test.py \
    --checkpoint output/dstark/best_model.pth \
    --data_root data/GOT10k \
    --benchmark GOT10k \
    --output_dir results/got10k \
    --gpu 0

# Test with visualization
python test.py \
    --checkpoint output/dstark/best_model.pth \
    --benchmark GOT10k \
    --visualize
```

## Advantages over Standard STARK

| Feature | STARK | DSTARK |
|---------|-------|--------|
| Template Size | Fixed (128Ã—128) | Flexible (128-256) |
| Search Size | Fixed (256Ã—256) | Flexible (256-512) |
| Backbone | ResNet-50 | DINOv3 Small |
| Position Encoding | Learnable | RoPE (rotation-based) |
| Feature Quality | Standard | Rich self-supervised |
| Occlusion Handling | Moderate | Improved |
| Scale Variation | Limited | Adaptive |

## Why DINOv3 with RoPE?

Traditional trackers struggle with:
- **Fixed size constraints**: 128Ã—128 template, 256Ã—256 search
- **Poor occlusion handling**: Features not distinctive enough
- **Scale variation issues**: Can't adapt to changing object sizes
- **Target switching**: May lock onto wrong object after occlusion

DINOv3 solves these with:
- **RoPE (Rotary Position Embeddings)**: Enables flexible input sizes
- **Rich features**: Better discrimination between objects
- **Self-supervised learning**: More robust representations
- **No size constraints**: Can process varying dimensions

## Project Structure

```
dstrak/
â”œâ”€â”€ dstark/                    # Main package
â”‚   â”œâ”€â”€ models/                # Model definitions
â”‚   â”‚   â”œâ”€â”€ dinov3_backbone.py # DINOv3 backbone
â”‚   â”‚   â””â”€â”€ dstark_tracker.py  # DSTARK tracker
â”‚   â”œâ”€â”€ data/                  # Data loading
â”‚   â”‚   â”œâ”€â”€ tracking_dataset.py
â”‚   â”‚   â””â”€â”€ sampler.py
â”‚   â”œâ”€â”€ lib/                   # Training utilities
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â””â”€â”€ train_utils.py
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ box_ops.py
â”‚       â””â”€â”€ misc.py
â”œâ”€â”€ configs/                   # Configuration files
â”‚   â”œâ”€â”€ dstark_train.yaml
â”‚   â””â”€â”€ dstark_test.yaml
â”œâ”€â”€ train.py                   # Training script
â”œâ”€â”€ test.py                    # Testing script
â”œâ”€â”€ dinov3_vits16_pretrain.pth # Pretrained weights
â””â”€â”€ README.md
```

## Citation

If you use DSTARK in your research, please cite:

```bibtex
@article{dstark2024,
  title={DSTARK: DINOv3-based STARK Tracker with Flexible Template and Search Sizes},
  author={Your Name},
  year={2024}
}
```

## Acknowledgements

- [STARK](https://github.com/researchmm/Stark) - Original STARK tracker
- [DINOv3](https://github.com/facebookresearch/dinov3) - DINOv3 self-supervised learning

## License

This project is released under the MIT License.
